{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":250645,"sourceType":"datasetVersion","datasetId":105271},{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os \nimport cv2 as cv \nimport glob\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler,Subset\nfrom torchvision import transforms\nfrom IPython.display import HTML\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom torchvision  import datasets \nfrom sklearn.model_selection import KFold\nimport optuna\nfrom optuna.trial import Trial\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchvision import models\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import Subset, DataLoader","metadata":{"_cell_guid":"b2ca991b-c0ce-4948-9114-7f2ab26bea1d","_uuid":"d021ba0b-d4b4-45f7-b07b-6256a3ff2d1a","collapsed":false,"execution":{"iopub.status.busy":"2024-12-25T13:40:07.264704Z","iopub.execute_input":"2024-12-25T13:40:07.265019Z","iopub.status.idle":"2024-12-25T13:40:11.200247Z","shell.execute_reply.started":"2024-12-25T13:40:07.264996Z","shell.execute_reply":"2024-12-25T13:40:11.199327Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:15.732381Z","iopub.execute_input":"2024-12-25T13:40:15.732801Z","iopub.status.idle":"2024-12-25T13:40:15.804207Z","shell.execute_reply.started":"2024-12-25T13:40:15.732774Z","shell.execute_reply":"2024-12-25T13:40:15.803404Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"combined_data = \"/kaggle/input/deepfake-and-real-images/Dataset/Test\"","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:17.123382Z","iopub.execute_input":"2024-12-25T13:40:17.123676Z","iopub.status.idle":"2024-12-25T13:40:17.127226Z","shell.execute_reply.started":"2024-12-25T13:40:17.123653Z","shell.execute_reply":"2024-12-25T13:40:17.126420Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224\n    transforms.ToTensor(),         # Convert to Tensor\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize\n])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:18.391262Z","iopub.execute_input":"2024-12-25T13:40:18.391537Z","iopub.status.idle":"2024-12-25T13:40:18.395727Z","shell.execute_reply.started":"2024-12-25T13:40:18.391517Z","shell.execute_reply":"2024-12-25T13:40:18.394937Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"combinedData = datasets.ImageFolder(combined_data, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:19.603146Z","iopub.execute_input":"2024-12-25T13:40:19.603424Z","iopub.status.idle":"2024-12-25T13:40:26.165103Z","shell.execute_reply.started":"2024-12-25T13:40:19.603404Z","shell.execute_reply":"2024-12-25T13:40:26.164195Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"combinedData","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:26.166100Z","iopub.execute_input":"2024-12-25T13:40:26.166309Z","iopub.status.idle":"2024-12-25T13:40:26.171714Z","shell.execute_reply.started":"2024-12-25T13:40:26.166291Z","shell.execute_reply":"2024-12-25T13:40:26.170949Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset ImageFolder\n    Number of datapoints: 10905\n    Root location: /kaggle/input/deepfake-and-real-images/Dataset/Test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n           )"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"kf = KFold(n_splits=3, shuffle=True, random_state=42) ","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:27.996328Z","iopub.execute_input":"2024-12-25T13:40:27.996604Z","iopub.status.idle":"2024-12-25T13:40:28.000305Z","shell.execute_reply.started":"2024-12-25T13:40:27.996584Z","shell.execute_reply":"2024-12-25T13:40:27.999432Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"samples = len(combinedData)\nsamples","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:29.155273Z","iopub.execute_input":"2024-12-25T13:40:29.155596Z","iopub.status.idle":"2024-12-25T13:40:29.160368Z","shell.execute_reply.started":"2024-12-25T13:40:29.155571Z","shell.execute_reply":"2024-12-25T13:40:29.159696Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"10905"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"folds = list(kf.split(np.arange(samples)))","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:29.615211Z","iopub.execute_input":"2024-12-25T13:40:29.615490Z","iopub.status.idle":"2024-12-25T13:40:29.621074Z","shell.execute_reply.started":"2024-12-25T13:40:29.615471Z","shell.execute_reply":"2024-12-25T13:40:29.620247Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"folds","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:30.168120Z","iopub.execute_input":"2024-12-25T13:40:30.168397Z","iopub.status.idle":"2024-12-25T13:40:30.174184Z","shell.execute_reply.started":"2024-12-25T13:40:30.168378Z","shell.execute_reply":"2024-12-25T13:40:30.173438Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[(array([    1,     2,     4, ..., 10900, 10901, 10902]),\n  array([    0,     3,     8, ..., 10899, 10903, 10904])),\n (array([    0,     1,     3, ..., 10902, 10903, 10904]),\n  array([    2,    15,    18, ..., 10889, 10900, 10901])),\n (array([    0,     2,     3, ..., 10901, 10903, 10904]),\n  array([    1,     4,     5, ..., 10896, 10897, 10902]))]"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Split our data into 3 Kfolds","metadata":{}},{"cell_type":"code","source":"for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(np.arange(samples))):\n    print(f\"Fold {fold_idx + 1}\")\n    \n    train_size = int(0.75* len(train_val_idx)) \n    val_size = len(train_val_idx) - train_size #25\n    \n    train_idx = train_val_idx[:train_size]\n    val_idx = train_val_idx[train_size:]\n    \n    # Create subsets\n    train_set = Subset(combinedData, train_idx)\n    val_set = Subset(combinedData, val_idx)\n    test_set = Subset(combinedData, test_idx)\n    \n    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n    \n    print(f\"Train size: {len(train_set)}\")\n    print(f\"Validation size: {len(val_set)}\")\n    print(f\"Test size: {len(test_set)}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-25T13:40:32.395759Z","iopub.execute_input":"2024-12-25T13:40:32.396096Z","iopub.status.idle":"2024-12-25T13:40:32.407579Z","shell.execute_reply.started":"2024-12-25T13:40:32.396069Z","shell.execute_reply":"2024-12-25T13:40:32.406658Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Fold 1\nTrain size: 5452\nValidation size: 1818\nTest size: 3635\nFold 2\nTrain size: 5452\nValidation size: 1818\nTest size: 3635\nFold 3\nTrain size: 5452\nValidation size: 1818\nTest size: 3635\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Define our model mobilenet","metadata":{}},{"cell_type":"code","source":"def get_model(name):\n    if name == \"densenet121\":\n        # Load the pretrained DenseNet121 model\n        model = models.densenet121(pretrained=True)\n        \n        # Freeze all layers\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Modify the classifier for the desired number of output classes\n        num_features = model.classifier.in_features\n        model.classifier = nn.Linear(num_features, 2)\n        \n        # Print the number of trainable parameters (should be only the classifier)\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Total trainable parameters: {trainable_params:,}\")\n    \n    else:\n        raise ValueError(\"Model name must be 'densenet121'\")\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:12:50.125975Z","iopub.execute_input":"2024-12-24T23:12:50.126257Z","iopub.status.idle":"2024-12-24T23:12:50.131119Z","shell.execute_reply.started":"2024-12-24T23:12:50.126236Z","shell.execute_reply":"2024-12-24T23:12:50.130083Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Training function","metadata":{}},{"cell_type":"code","source":"def TrainingModels(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n        images, labels = images.to(device), labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)  #loss function used to compute the error between predictions and true labels\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1) # max score from the scores of two classes\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:12:52.241474Z","iopub.execute_input":"2024-12-24T23:12:52.241796Z","iopub.status.idle":"2024-12-24T23:12:52.247178Z","shell.execute_reply.started":"2024-12-24T23:12:52.241773Z","shell.execute_reply":"2024-12-24T23:12:52.246230Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Eval function","metadata":{}},{"cell_type":"code","source":"def evaluate(model, data_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:13:00.096649Z","iopub.execute_input":"2024-12-24T23:13:00.096955Z","iopub.status.idle":"2024-12-24T23:13:00.102461Z","shell.execute_reply.started":"2024-12-24T23:13:00.096934Z","shell.execute_reply":"2024-12-24T23:13:00.101620Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Optimize the Learning rate using Optuna","metadata":{}},{"cell_type":"code","source":"def LR_optimization(trial: Trial):\n    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n    \n    # Iterate over folds\n    fold_results = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(np.arange(samples))):\n        print(f\"\\nFold {fold_idx + 1}\")\n        \n        train_size = int(0.75 * len(train_val_idx))\n        train_idx = train_val_idx[:train_size]\n        val_idx = train_val_idx[train_size:]\n\n        train_set = Subset(combinedData, train_idx)\n        val_set = Subset(combinedData, val_idx)\n        test_set = Subset(combinedData, test_idx)\n\n        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\n        \n        for model_name in [\"densenet121\"]:\n            print(f\"\\nTraining model: {model_name.upper()} on Fold {fold_idx + 1}\")\n            \n            # Initialize model\n            model = get_model(model_name).to(device)\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(model.parameters(), lr=lr)\n\n            for epoch in range(5):\n                current_lr = optimizer.param_groups[0][\"lr\"]\n                print(f\"\\nEpoch {epoch + 1}/5 | Learning Rate: {current_lr:.6f}\")\n                \n                # Train and evaluate\n                train_loss, train_acc = TrainingModels(model, train_loader, criterion, optimizer)\n                val_loss, val_acc = evaluate(model, val_loader, criterion)\n\n                print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n                print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n            \n            _, val_acc = evaluate(model, val_loader, criterion)\n            fold_results.append(val_acc) \n\n    mean_val_acc = sum(fold_results) / len(fold_results)\n    return mean_val_acc\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:13:06.816821Z","iopub.execute_input":"2024-12-24T23:13:06.817137Z","iopub.status.idle":"2024-12-24T23:13:06.824480Z","shell.execute_reply.started":"2024-12-24T23:13:06.817111Z","shell.execute_reply":"2024-12-24T23:13:06.823485Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Without Fine-tune ","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(LR_optimization, n_trials=5)\n\nbest_trial = study.best_trial\nprint(\"\\nBest Trial:\")\nprint(f\"  Value (Mean Validation Accuracy): {best_trial.value}\")\nprint(f\"  Params: {best_trial.params}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-24T23:13:10.154712Z","iopub.execute_input":"2024-12-24T23:13:10.155027Z","iopub.status.idle":"2024-12-25T00:09:57.423815Z","shell.execute_reply.started":"2024-12-24T23:13:10.154999Z","shell.execute_reply":"2024-12-25T00:09:57.422765Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[I 2024-12-24 23:13:10,156] A new study created in memory with name: no-name-be131352-c94a-438d-ad7c-418862414302\n<ipython-input-16-3906ab1e677d>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nTraining model: DENSENET121 on Fold 1\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 168MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6601, Train Acc: 0.6328\nVal Loss: 1.0801, Val Acc: 0.0776\n\nEpoch 2/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6465, Train Acc: 0.6548\nVal Loss: 1.1182, Val Acc: 0.0611\n\nEpoch 3/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6418, Train Acc: 0.6588\nVal Loss: 1.0957, Val Acc: 0.0737\n\nEpoch 4/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6356, Train Acc: 0.6585\nVal Loss: 1.1421, Val Acc: 0.0556\n\nEpoch 5/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6292, Train Acc: 0.6609\nVal Loss: 1.0899, Val Acc: 0.0803\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nTraining model: DENSENET121 on Fold 2\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7173, Train Acc: 0.5193\nVal Loss: 1.0413, Val Acc: 0.0990\n\nEpoch 2/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6540, Train Acc: 0.6515\nVal Loss: 1.1757, Val Acc: 0.0440\n\nEpoch 3/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6453, Train Acc: 0.6676\nVal Loss: 1.1447, Val Acc: 0.0512\n\nEpoch 4/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6413, Train Acc: 0.6638\nVal Loss: 1.1321, Val Acc: 0.0594\n\nEpoch 5/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6362, Train Acc: 0.6651\nVal Loss: 1.1031, Val Acc: 0.0737\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nTraining model: DENSENET121 on Fold 3\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6571, Train Acc: 0.6717\nVal Loss: 1.1801, Val Acc: 0.0330\n\nEpoch 2/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6467, Train Acc: 0.6686\nVal Loss: 1.1885, Val Acc: 0.0336\n\nEpoch 3/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6415, Train Acc: 0.6697\nVal Loss: 1.1539, Val Acc: 0.0468\n\nEpoch 4/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6374, Train Acc: 0.6706\nVal Loss: 1.1154, Val Acc: 0.0605\n\nEpoch 5/5 | Learning Rate: 0.000013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6281, Train Acc: 0.6728\nVal Loss: 1.0981, Val Acc: 0.0666\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-24 23:25:02,133] Trial 0 finished with value: 0.07352401906857353 and parameters: {'lr': 1.2928385990792672e-05}. Best is trial 0 with value: 0.07352401906857353.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nTraining model: DENSENET121 on Fold 1\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6394, Train Acc: 0.6453\nVal Loss: 0.8614, Val Acc: 0.2822\n\nEpoch 2/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5927, Train Acc: 0.6834\nVal Loss: 0.9822, Val Acc: 0.1744\n\nEpoch 3/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5635, Train Acc: 0.6985\nVal Loss: 0.8534, Val Acc: 0.3498\n\nEpoch 4/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5494, Train Acc: 0.7102\nVal Loss: 0.8284, Val Acc: 0.4015\n\nEpoch 5/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5311, Train Acc: 0.7232\nVal Loss: 0.9331, Val Acc: 0.3097\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nTraining model: DENSENET121 on Fold 2\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6440, Train Acc: 0.6526\nVal Loss: 1.1481, Val Acc: 0.0556\n\nEpoch 2/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5879, Train Acc: 0.6856\nVal Loss: 1.0446, Val Acc: 0.1392\n\nEpoch 3/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5607, Train Acc: 0.7060\nVal Loss: 0.8956, Val Acc: 0.3003\n\nEpoch 4/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5429, Train Acc: 0.7177\nVal Loss: 0.7642, Val Acc: 0.4769\n\nEpoch 5/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5253, Train Acc: 0.7340\nVal Loss: 0.8588, Val Acc: 0.3806\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nTraining model: DENSENET121 on Fold 3\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6188, Train Acc: 0.6766\nVal Loss: 0.9115, Val Acc: 0.2178\n\nEpoch 2/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5740, Train Acc: 0.6942\nVal Loss: 0.9445, Val Acc: 0.2294\n\nEpoch 3/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5484, Train Acc: 0.7161\nVal Loss: 1.0259, Val Acc: 0.1870\n\nEpoch 4/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5309, Train Acc: 0.7285\nVal Loss: 0.9468, Val Acc: 0.2866\n\nEpoch 5/5 | Learning Rate: 0.000139\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5172, Train Acc: 0.7392\nVal Loss: 0.9210, Val Acc: 0.3278\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-24 23:36:08,042] Trial 1 finished with value: 0.33938393839383935 and parameters: {'lr': 0.0001388121887079759}. Best is trial 1 with value: 0.33938393839383935.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nTraining model: DENSENET121 on Fold 1\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6280, Train Acc: 0.6618\nVal Loss: 0.9725, Val Acc: 0.1524\n\nEpoch 2/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5829, Train Acc: 0.6867\nVal Loss: 0.9469, Val Acc: 0.2206\n\nEpoch 3/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5572, Train Acc: 0.7036\nVal Loss: 0.8788, Val Acc: 0.3229\n\nEpoch 4/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5403, Train Acc: 0.7208\nVal Loss: 0.8448, Val Acc: 0.3746\n\nEpoch 5/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5279, Train Acc: 0.7247\nVal Loss: 0.8098, Val Acc: 0.4340\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nTraining model: DENSENET121 on Fold 2\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6455, Train Acc: 0.6568\nVal Loss: 0.9559, Val Acc: 0.1562\n\nEpoch 2/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5953, Train Acc: 0.6768\nVal Loss: 0.9118, Val Acc: 0.2442\n\nEpoch 3/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5661, Train Acc: 0.7040\nVal Loss: 0.9475, Val Acc: 0.2437\n\nEpoch 4/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5461, Train Acc: 0.7100\nVal Loss: 1.0280, Val Acc: 0.2074\n\nEpoch 5/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5324, Train Acc: 0.7285\nVal Loss: 0.8068, Val Acc: 0.4521\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nTraining model: DENSENET121 on Fold 3\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6392, Train Acc: 0.6548\nVal Loss: 1.1623, Val Acc: 0.0561\n\nEpoch 2/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5887, Train Acc: 0.6864\nVal Loss: 1.0566, Val Acc: 0.1227\n\nEpoch 3/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5617, Train Acc: 0.7023\nVal Loss: 1.0272, Val Acc: 0.1777\n\nEpoch 4/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5437, Train Acc: 0.7183\nVal Loss: 0.9893, Val Acc: 0.2503\n\nEpoch 5/5 | Learning Rate: 0.000133\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5326, Train Acc: 0.7278\nVal Loss: 0.8685, Val Acc: 0.3795\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-24 23:47:19,577] Trial 2 finished with value: 0.42189218921892185 and parameters: {'lr': 0.00013334284716503883}. Best is trial 2 with value: 0.42189218921892185.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nTraining model: DENSENET121 on Fold 1\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6216, Train Acc: 0.6697\nVal Loss: 0.9383, Val Acc: 0.2442\n\nEpoch 2/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5721, Train Acc: 0.6922\nVal Loss: 0.9178, Val Acc: 0.2915\n\nEpoch 3/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5482, Train Acc: 0.7133\nVal Loss: 0.9495, Val Acc: 0.2871\n\nEpoch 4/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5292, Train Acc: 0.7271\nVal Loss: 0.9645, Val Acc: 0.3031\n\nEpoch 5/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5123, Train Acc: 0.7428\nVal Loss: 0.8158, Val Acc: 0.4532\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nTraining model: DENSENET121 on Fold 2\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6146, Train Acc: 0.6684\nVal Loss: 1.0827, Val Acc: 0.1293\n\nEpoch 2/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5662, Train Acc: 0.6979\nVal Loss: 0.8977, Val Acc: 0.3185\n\nEpoch 3/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5427, Train Acc: 0.7142\nVal Loss: 0.8268, Val Acc: 0.4186\n\nEpoch 4/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5224, Train Acc: 0.7307\nVal Loss: 1.0514, Val Acc: 0.2497\n\nEpoch 5/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5138, Train Acc: 0.7392\nVal Loss: 1.0349, Val Acc: 0.2728\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nTraining model: DENSENET121 on Fold 3\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6139, Train Acc: 0.6735\nVal Loss: 0.8529, Val Acc: 0.3163\n\nEpoch 2/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5646, Train Acc: 0.7076\nVal Loss: 0.8219, Val Acc: 0.3933\n\nEpoch 3/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5379, Train Acc: 0.7219\nVal Loss: 0.9878, Val Acc: 0.2706\n\nEpoch 4/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5224, Train Acc: 0.7313\nVal Loss: 0.9028, Val Acc: 0.3564\n\nEpoch 5/5 | Learning Rate: 0.000194\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5088, Train Acc: 0.7500\nVal Loss: 0.7348, Val Acc: 0.5226\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-24 23:58:42,457] Trial 3 finished with value: 0.4162082874954162 and parameters: {'lr': 0.00019420417875367233}. Best is trial 2 with value: 0.42189218921892185.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nTraining model: DENSENET121 on Fold 1\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5839, Train Acc: 0.6825\nVal Loss: 0.8342, Val Acc: 0.4004\n\nEpoch 2/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5216, Train Acc: 0.7353\nVal Loss: 0.7043, Val Acc: 0.5671\n\nEpoch 3/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5019, Train Acc: 0.7456\nVal Loss: 0.5921, Val Acc: 0.6799\n\nEpoch 4/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4859, Train Acc: 0.7539\nVal Loss: 0.7727, Val Acc: 0.5193\n\nEpoch 5/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4749, Train Acc: 0.7630\nVal Loss: 0.8163, Val Acc: 0.4895\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nTraining model: DENSENET121 on Fold 2\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5913, Train Acc: 0.6851\nVal Loss: 0.8500, Val Acc: 0.3933\n\nEpoch 2/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5291, Train Acc: 0.7278\nVal Loss: 0.6237, Val Acc: 0.6480\n\nEpoch 3/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5036, Train Acc: 0.7472\nVal Loss: 0.8364, Val Acc: 0.4527\n\nEpoch 4/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4824, Train Acc: 0.7634\nVal Loss: 0.9785, Val Acc: 0.3559\n\nEpoch 5/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4754, Train Acc: 0.7623\nVal Loss: 0.8806, Val Acc: 0.4527\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nTraining model: DENSENET121 on Fold 3\nTotal trainable parameters: 2,050\n\nEpoch 1/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5762, Train Acc: 0.6988\nVal Loss: 0.7886, Val Acc: 0.4736\n\nEpoch 2/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5204, Train Acc: 0.7350\nVal Loss: 1.0680, Val Acc: 0.2591\n\nEpoch 3/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5048, Train Acc: 0.7405\nVal Loss: 0.9120, Val Acc: 0.3971\n\nEpoch 4/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4862, Train Acc: 0.7594\nVal Loss: 0.9517, Val Acc: 0.3872\n\nEpoch 5/5 | Learning Rate: 0.000545\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4749, Train Acc: 0.7639\nVal Loss: 0.8540, Val Acc: 0.4692\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 00:09:57,418] Trial 4 finished with value: 0.47048038137147047 and parameters: {'lr': 0.0005446615699330324}. Best is trial 4 with value: 0.47048038137147047.\n","output_type":"stream"},{"name":"stdout","text":"\nBest Trial:\n  Value (Mean Validation Accuracy): 0.47048038137147047\n  Params: {'lr': 0.0005446615699330324}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# After the Optuna optimization process, we will use the best trial to set the learning rate","metadata":{}},{"cell_type":"code","source":"# Best learning rate from Optuna\nbest_lr = 0.0005446615699330324","metadata":{"execution":{"iopub.status.busy":"2024-12-25T00:11:18.189493Z","iopub.execute_input":"2024-12-25T00:11:18.189817Z","iopub.status.idle":"2024-12-25T00:11:18.193212Z","shell.execute_reply.started":"2024-12-25T00:11:18.189793Z","shell.execute_reply":"2024-12-25T00:11:18.192485Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"final_results = {}\n\nfor model_name in [\"densenet121\"]:\n    print(f\"\\nTraining model: {model_name.upper()} with best learning rate {best_lr}\")\n    \n    # Initialize model\n    model = get_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    \n    # Create DataLoaders (assuming combinedData is split into train and test datasets)\n    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\n    # Train the model\n    for epoch in range(10):  # Example: 10 epochs\n        print(f\"\\nEpoch {epoch + 1}/10\")\n        \n        # Train and evaluate\n        train_loss, train_acc = TrainingModels(model, train_loader, criterion, optimizer)\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    \n    # Final evaluation on the test set\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n    \n    # Store results\n    final_results[model_name] = {\n        \"Test Loss\": test_loss,\n        \"Test Accuracy\": test_acc\n    }\n\n# Print final results\nprint(\"\\nFinal Results:\")\nfor model_name, metrics in final_results.items():\n    print(f\"{model_name.upper()}:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-25T00:11:33.622886Z","iopub.execute_input":"2024-12-25T00:11:33.623165Z","iopub.status.idle":"2024-12-25T00:17:11.530295Z","shell.execute_reply.started":"2024-12-25T00:11:33.623143Z","shell.execute_reply":"2024-12-25T00:17:11.529537Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nTraining model: DENSENET121 with best learning rate 0.0005446615699330324\nTotal trainable parameters: 2,050\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5733, Train Acc: 0.7040\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5172, Train Acc: 0.7368\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4975, Train Acc: 0.7550\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4865, Train Acc: 0.7603\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4677, Train Acc: 0.7678\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4670, Train Acc: 0.7755\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4624, Train Acc: 0.7729\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4627, Train Acc: 0.7784\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4470, Train Acc: 0.7900\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4437, Train Acc: 0.7863\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5409, Test Acc: 0.7114\n\nFinal Results:\nDENSENET121:\n  Test Loss: 0.5409\n  Test Accuracy: 0.7114\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Function to evaluate the model and get performance metrics\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\n# Initialize cross-validation metrics storage\nconfusion_matrices = []\naccuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\n\nbest_lr = 0.0005446615699330324\n\nfor model_name in [\"densenet121\"]:\n    print(f\"\\nTraining model: {model_name.upper()} with best learning rate {best_lr}\")\n    \n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(np.arange(samples))):\n        print(f\"Fold {fold_idx + 1}\")\n\n        train_size = int(0.75 * len(train_val_idx))\n        train_idx = train_val_idx[:train_size]\n        val_idx = train_val_idx[train_size:]\n\n        train_set = Subset(combinedData, train_idx)\n        val_set = Subset(combinedData, val_idx)\n        test_set = Subset(combinedData, test_idx)\n\n        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n        \n        \n        model = get_model(model_name).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=best_lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # Train the model\n        for epoch in range(10):\n            model.train()\n            running_loss = 0.0\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}/10, Loss: {running_loss / len(train_loader):.4f}\")\n\n    \n        preds, labels = evaluate_model(model, test_loader)\n\n        \n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average='weighted', zero_division=0)\n        recall = recall_score(labels, preds, average='weighted', zero_division=0)\n        f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n        cm = confusion_matrix(labels, preds)\n\n        # Store metrics for the fold\n        confusion_matrices.append(cm)\n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n\n        print(f\"Fold {fold_idx + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n        print(f\"Confusion Matrix:\\n{cm}\\n\")\n\n    # Average metrics across folds\n    avg_accuracy = np.mean(accuracies)\n    avg_precision = np.mean(precisions)\n    avg_recall = np.mean(recalls)\n    avg_f1_score = np.mean(f1_scores)\n    avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n\n    # Print final average metrics for the model\n    print(f\"\\nFinal Results for {model_name.upper()}:\")\n    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n    print(f\"Average Precision: {avg_precision:.4f}\")\n    print(f\"Average Recall: {avg_recall:.4f}\")\n    print(f\"Average F1-Score: {avg_f1_score:.4f}\")\n    print(f\"Average Confusion Matrix:\\n{avg_confusion_matrix}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-25T00:21:07.883171Z","iopub.execute_input":"2024-12-25T00:21:07.883490Z","iopub.status.idle":"2024-12-25T00:38:06.923076Z","shell.execute_reply.started":"2024-12-25T00:21:07.883461Z","shell.execute_reply":"2024-12-25T00:38:06.922314Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nTraining model: DENSENET121 with best learning rate 0.0005446615699330324\nFold 1\nTotal trainable parameters: 2,050\nEpoch 1/10, Loss: 0.5823\nEpoch 2/10, Loss: 0.5212\nEpoch 3/10, Loss: 0.5043\nEpoch 4/10, Loss: 0.4821\nEpoch 5/10, Loss: 0.4778\nEpoch 6/10, Loss: 0.4746\nEpoch 7/10, Loss: 0.4670\nEpoch 8/10, Loss: 0.4586\nEpoch 9/10, Loss: 0.4575\nEpoch 10/10, Loss: 0.4511\nFold 1 - Accuracy: 0.7004, Precision: 0.7223, Recall: 0.7004, F1-Score: 0.6907\nConfusion Matrix:\n[[1618  246]\n [ 843  928]]\n\nFold 2\nTotal trainable parameters: 2,050\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.5737\nEpoch 2/10, Loss: 0.5114\nEpoch 3/10, Loss: 0.4935\nEpoch 4/10, Loss: 0.4799\nEpoch 5/10, Loss: 0.4688\nEpoch 6/10, Loss: 0.4638\nEpoch 7/10, Loss: 0.4580\nEpoch 8/10, Loss: 0.4474\nEpoch 9/10, Loss: 0.4507\nEpoch 10/10, Loss: 0.4444\nFold 2 - Accuracy: 0.6894, Precision: 0.7208, Recall: 0.6894, F1-Score: 0.6770\nConfusion Matrix:\n[[1617  217]\n [ 912  889]]\n\nFold 3\nTotal trainable parameters: 2,050\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.5868\nEpoch 2/10, Loss: 0.5229\nEpoch 3/10, Loss: 0.4980\nEpoch 4/10, Loss: 0.4860\nEpoch 5/10, Loss: 0.4726\nEpoch 6/10, Loss: 0.4720\nEpoch 7/10, Loss: 0.4710\nEpoch 8/10, Loss: 0.4616\nEpoch 9/10, Loss: 0.4520\nEpoch 10/10, Loss: 0.4486\nFold 3 - Accuracy: 0.6897, Precision: 0.7215, Recall: 0.6897, F1-Score: 0.6794\nConfusion Matrix:\n[[1568  226]\n [ 902  939]]\n\n\nFinal Results for DENSENET121:\nAverage Accuracy: 0.6932\nAverage Precision: 0.7215\nAverage Recall: 0.6932\nAverage F1-Score: 0.6824\nAverage Confusion Matrix:\n[[1601.          229.66666667]\n [ 885.66666667  918.66666667]]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Fine-tuning ","metadata":{}},{"cell_type":"markdown","source":"## Applying Fine-tune","metadata":{}},{"cell_type":"code","source":"def get_model(name):\n    if name == \"densenet121\":\n        # Load the pretrained DenseNet121 model\n        model = models.densenet121(pretrained=True)\n        \n        # Freeze all layers initially\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Fine-tune the last 5 layers:\n        # Unfreeze the last 5 dense blocks and transition layers\n        for param in model.features[-5:].parameters():\n            param.requires_grad = True\n        \n        # Modify the classifier for the desired number of output classes\n        num_features = model.classifier.in_features\n        model.classifier = nn.Linear(num_features, 2)\n        \n        # Ensure the classifier is trainable\n        for param in model.classifier.parameters():\n            param.requires_grad = True\n        \n        # Print the number of trainable parameters\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Total trainable parameters: {trainable_params:,}\")\n    \n    else:\n        raise ValueError(\"Model name must be 'densenet121'\")\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:48.176226Z","iopub.execute_input":"2024-12-25T13:40:48.176535Z","iopub.status.idle":"2024-12-25T13:40:48.181682Z","shell.execute_reply.started":"2024-12-25T13:40:48.176510Z","shell.execute_reply":"2024-12-25T13:40:48.180961Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Training Function ","metadata":{}},{"cell_type":"code","source":"# Fine-tune\n\ndef TrainingModels(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # Add gradient clipping to prevent exploding gradients\n    max_grad_norm = 1.0\n    \n    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n        images, labels = images.to(device), labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        \n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:50.303127Z","iopub.execute_input":"2024-12-25T13:40:50.303424Z","iopub.status.idle":"2024-12-25T13:40:50.308694Z","shell.execute_reply.started":"2024-12-25T13:40:50.303402Z","shell.execute_reply":"2024-12-25T13:40:50.307849Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate(model, data_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:51.932730Z","iopub.execute_input":"2024-12-25T13:40:51.933074Z","iopub.status.idle":"2024-12-25T13:40:51.938294Z","shell.execute_reply.started":"2024-12-25T13:40:51.933047Z","shell.execute_reply":"2024-12-25T13:40:51.937410Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Fine-tune\n\ndef LR_optimization(trial: Trial):\n    # Use a smaller learning rate range for fine-tuning\n    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)\n    \n    # Rest of the function remains the same\n    fold_results = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(np.arange(samples))):\n        print(f\"\\nFold {fold_idx + 1}\")\n        \n        train_size = int(0.75 * len(train_val_idx))\n        train_idx = train_val_idx[:train_size]\n        val_idx = train_val_idx[train_size:]\n\n        train_set = Subset(combinedData, train_idx)\n        val_set = Subset(combinedData, val_idx)\n        test_set = Subset(combinedData, test_idx)\n\n        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\n        for model_name in [\"densenet121\"]:\n            print(f\"\\nFine-tuning model: {model_name.upper()} on Fold {fold_idx + 1}\")\n            \n            model = get_model(model_name).to(device)\n            criterion = nn.CrossEntropyLoss()\n            \n            # Use a smaller weight decay for fine-tuning\n            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n            \n            # Add learning rate scheduler for fine-tuning\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, mode='max', factor=0.5, patience=2, verbose=True\n            )\n\n            for epoch in range(5):\n                current_lr = optimizer.param_groups[0][\"lr\"]\n                print(f\"\\nEpoch {epoch + 1}/5 | Learning Rate: {current_lr:.6f}\")\n                \n                train_loss, train_acc = TrainingModels(model, train_loader, criterion, optimizer)\n                val_loss, val_acc = evaluate(model, val_loader, criterion)\n                \n                # Update learning rate based on validation accuracy\n                scheduler.step(val_acc)\n\n                print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n                print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n            \n            _, val_acc = evaluate(model, val_loader, criterion)\n            fold_results.append(val_acc)\n\n    mean_val_acc = sum(fold_results) / len(fold_results)\n    return mean_val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:53.561213Z","iopub.execute_input":"2024-12-25T13:40:53.561633Z","iopub.status.idle":"2024-12-25T13:40:53.569523Z","shell.execute_reply.started":"2024-12-25T13:40:53.561590Z","shell.execute_reply":"2024-12-25T13:40:53.568591Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(LR_optimization, n_trials=8)\n\nbest_trial = study.best_trial\nprint(\"\\nBest Trial:\")\nprint(f\"  Value (Mean Validation Accuracy): {best_trial.value}\")\nprint(f\"  Params: {best_trial.params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:40:56.636538Z","iopub.execute_input":"2024-12-25T13:40:56.636861Z","iopub.status.idle":"2024-12-25T15:35:10.039288Z","shell.execute_reply.started":"2024-12-25T13:40:56.636836Z","shell.execute_reply":"2024-12-25T15:35:10.038471Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-25 13:40:56,638] A new study created in memory with name: no-name-ddf37ecc-740e-4749-83c4-30d2cad80b3f\n<ipython-input-16-18e5c9fed5ec>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30.8M/30.8M [00:00<00:00, 156MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 5,658,370\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2955, Train Acc: 0.8773\nVal Loss: 0.3675, Val Acc: 0.8443\n\nEpoch 2/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1443, Train Acc: 0.9453\nVal Loss: 0.4463, Val Acc: 0.8163\n\nEpoch 3/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0848, Train Acc: 0.9699\nVal Loss: 0.4087, Val Acc: 0.8531\n\nEpoch 4/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0608, Train Acc: 0.9793\nVal Loss: 0.2113, Val Acc: 0.9285\n\nEpoch 5/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0352, Train Acc: 0.9875\nVal Loss: 0.4684, Val Acc: 0.8784\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2903, Train Acc: 0.8780\nVal Loss: 0.4120, Val Acc: 0.7998\n\nEpoch 2/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1379, Train Acc: 0.9481\nVal Loss: 0.7887, Val Acc: 0.7382\n\nEpoch 3/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0901, Train Acc: 0.9670\nVal Loss: 0.3737, Val Acc: 0.8889\n\nEpoch 4/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0443, Train Acc: 0.9848\nVal Loss: 0.1914, Val Acc: 0.9422\n\nEpoch 5/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0395, Train Acc: 0.9840\nVal Loss: 0.4427, Val Acc: 0.9032\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3037, Train Acc: 0.8731\nVal Loss: 0.3181, Val Acc: 0.8559\n\nEpoch 2/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1474, Train Acc: 0.9426\nVal Loss: 0.3701, Val Acc: 0.8388\n\nEpoch 3/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0735, Train Acc: 0.9708\nVal Loss: 0.6620, Val Acc: 0.7679\n\nEpoch 4/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0458, Train Acc: 0.9848\nVal Loss: 0.2340, Val Acc: 0.9296\n\nEpoch 5/5 | Learning Rate: 0.000378\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0463, Train Acc: 0.9844\nVal Loss: 0.3963, Val Acc: 0.8817\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 13:55:43,769] Trial 0 finished with value: 0.8877887788778877 and parameters: {'lr': 0.00037829587205874174}. Best is trial 0 with value: 0.8877887788778877.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3002, Train Acc: 0.8703\nVal Loss: 0.6989, Val Acc: 0.6969\n\nEpoch 2/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1607, Train Acc: 0.9395\nVal Loss: 0.7648, Val Acc: 0.7448\n\nEpoch 3/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1122, Train Acc: 0.9573\nVal Loss: 0.6598, Val Acc: 0.7283\n\nEpoch 4/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0731, Train Acc: 0.9741\nVal Loss: 0.5244, Val Acc: 0.8135\n\nEpoch 5/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0504, Train Acc: 0.9818\nVal Loss: 0.1643, Val Acc: 0.9417\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3027, Train Acc: 0.8793\nVal Loss: 1.4339, Val Acc: 0.5297\n\nEpoch 2/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1769, Train Acc: 0.9375\nVal Loss: 0.1021, Val Acc: 0.9637\n\nEpoch 3/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1132, Train Acc: 0.9578\nVal Loss: 0.2142, Val Acc: 0.9103\n\nEpoch 4/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0750, Train Acc: 0.9729\nVal Loss: 0.5003, Val Acc: 0.8091\n\nEpoch 5/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0566, Train Acc: 0.9785\nVal Loss: 0.3600, Val Acc: 0.8713\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3284, Train Acc: 0.8564\nVal Loss: 0.3645, Val Acc: 0.8564\n\nEpoch 2/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1765, Train Acc: 0.9334\nVal Loss: 0.2270, Val Acc: 0.9109\n\nEpoch 3/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1062, Train Acc: 0.9582\nVal Loss: 0.1924, Val Acc: 0.9175\n\nEpoch 4/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0768, Train Acc: 0.9697\nVal Loss: 0.3015, Val Acc: 0.8944\n\nEpoch 5/5 | Learning Rate: 0.000826\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0507, Train Acc: 0.9817\nVal Loss: 0.4094, Val Acc: 0.8608\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 14:09:51,854] Trial 1 finished with value: 0.8912724605793914 and parameters: {'lr': 0.0008263872006727257}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4697, Train Acc: 0.7581\nVal Loss: 0.3924, Val Acc: 0.8031\n\nEpoch 2/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1870, Train Acc: 0.9250\nVal Loss: 0.3745, Val Acc: 0.8218\n\nEpoch 3/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0863, Train Acc: 0.9690\nVal Loss: 0.5690, Val Acc: 0.7816\n\nEpoch 4/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0394, Train Acc: 0.9883\nVal Loss: 0.5057, Val Acc: 0.8427\n\nEpoch 5/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0251, Train Acc: 0.9928\nVal Loss: 0.5588, Val Acc: 0.8504\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4255, Train Acc: 0.8023\nVal Loss: 0.4398, Val Acc: 0.7866\n\nEpoch 2/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1755, Train Acc: 0.9310\nVal Loss: 0.4109, Val Acc: 0.8064\n\nEpoch 3/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0866, Train Acc: 0.9677\nVal Loss: 0.5673, Val Acc: 0.7937\n\nEpoch 4/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0358, Train Acc: 0.9870\nVal Loss: 0.5802, Val Acc: 0.8410\n\nEpoch 5/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0194, Train Acc: 0.9941\nVal Loss: 0.7305, Val Acc: 0.7937\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4319, Train Acc: 0.7847\nVal Loss: 0.4765, Val Acc: 0.7288\n\nEpoch 2/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1796, Train Acc: 0.9272\nVal Loss: 0.4416, Val Acc: 0.7855\n\nEpoch 3/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0825, Train Acc: 0.9681\nVal Loss: 0.4607, Val Acc: 0.8108\n\nEpoch 4/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0419, Train Acc: 0.9850\nVal Loss: 0.5905, Val Acc: 0.8119\n\nEpoch 5/5 | Learning Rate: 0.000035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0251, Train Acc: 0.9927\nVal Loss: 0.5690, Val Acc: 0.8465\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 14:24:12,436] Trial 2 finished with value: 0.8302163549688303 and parameters: {'lr': 3.497952531434269e-05}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3035, Train Acc: 0.8626\nVal Loss: 0.4611, Val Acc: 0.8108\n\nEpoch 2/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1174, Train Acc: 0.9556\nVal Loss: 0.3202, Val Acc: 0.8861\n\nEpoch 3/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0594, Train Acc: 0.9800\nVal Loss: 0.5275, Val Acc: 0.8438\n\nEpoch 4/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0313, Train Acc: 0.9890\nVal Loss: 0.4896, Val Acc: 0.8889\n\nEpoch 5/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0341, Train Acc: 0.9903\nVal Loss: 0.6899, Val Acc: 0.8658\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3122, Train Acc: 0.8628\nVal Loss: 0.4717, Val Acc: 0.7816\n\nEpoch 2/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1120, Train Acc: 0.9587\nVal Loss: 0.6201, Val Acc: 0.8185\n\nEpoch 3/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0533, Train Acc: 0.9809\nVal Loss: 0.8649, Val Acc: 0.8168\n\nEpoch 4/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0392, Train Acc: 0.9897\nVal Loss: 0.6381, Val Acc: 0.8542\n\nEpoch 5/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0268, Train Acc: 0.9916\nVal Loss: 0.3811, Val Acc: 0.8988\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3128, Train Acc: 0.8586\nVal Loss: 0.3860, Val Acc: 0.8190\n\nEpoch 2/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1186, Train Acc: 0.9545\nVal Loss: 0.3635, Val Acc: 0.8454\n\nEpoch 3/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0617, Train Acc: 0.9778\nVal Loss: 0.5170, Val Acc: 0.8454\n\nEpoch 4/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0359, Train Acc: 0.9875\nVal Loss: 0.6870, Val Acc: 0.8394\n\nEpoch 5/5 | Learning Rate: 0.000148\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0282, Train Acc: 0.9925\nVal Loss: 0.9363, Val Acc: 0.8025\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 14:38:15,401] Trial 3 finished with value: 0.8557022368903557 and parameters: {'lr': 0.00014767405285438304}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4692, Train Acc: 0.7650\nVal Loss: 0.5031, Val Acc: 0.7585\n\nEpoch 2/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2248, Train Acc: 0.9107\nVal Loss: 0.4594, Val Acc: 0.7684\n\nEpoch 3/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1148, Train Acc: 0.9589\nVal Loss: 0.5440, Val Acc: 0.7712\n\nEpoch 4/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0666, Train Acc: 0.9769\nVal Loss: 0.4213, Val Acc: 0.8328\n\nEpoch 5/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0329, Train Acc: 0.9894\nVal Loss: 0.6080, Val Acc: 0.8141\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4805, Train Acc: 0.7566\nVal Loss: 0.5877, Val Acc: 0.6606\n\nEpoch 2/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2224, Train Acc: 0.9154\nVal Loss: 0.5385, Val Acc: 0.7338\n\nEpoch 3/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1174, Train Acc: 0.9558\nVal Loss: 0.4874, Val Acc: 0.7882\n\nEpoch 4/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0578, Train Acc: 0.9811\nVal Loss: 0.5117, Val Acc: 0.8130\n\nEpoch 5/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0293, Train Acc: 0.9905\nVal Loss: 0.5409, Val Acc: 0.8377\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4709, Train Acc: 0.7727\nVal Loss: 0.5569, Val Acc: 0.6892\n\nEpoch 2/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2249, Train Acc: 0.9120\nVal Loss: 0.3928, Val Acc: 0.8025\n\nEpoch 3/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1291, Train Acc: 0.9529\nVal Loss: 0.5010, Val Acc: 0.7838\n\nEpoch 4/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0618, Train Acc: 0.9798\nVal Loss: 0.4833, Val Acc: 0.8119\n\nEpoch 5/5 | Learning Rate: 0.000027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0332, Train Acc: 0.9897\nVal Loss: 0.4452, Val Acc: 0.8460\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 14:52:30,472] Trial 4 finished with value: 0.8325999266593326 and parameters: {'lr': 2.7030608478958514e-05}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4920, Train Acc: 0.7476\nVal Loss: 0.5990, Val Acc: 0.6722\n\nEpoch 2/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2396, Train Acc: 0.9035\nVal Loss: 0.4332, Val Acc: 0.7904\n\nEpoch 3/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1399, Train Acc: 0.9468\nVal Loss: 0.4438, Val Acc: 0.7899\n\nEpoch 4/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0802, Train Acc: 0.9743\nVal Loss: 0.5854, Val Acc: 0.7728\n\nEpoch 5/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0472, Train Acc: 0.9848\nVal Loss: 0.7278, Val Acc: 0.7624\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4625, Train Acc: 0.7795\nVal Loss: 0.6095, Val Acc: 0.6441\n\nEpoch 2/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2259, Train Acc: 0.9142\nVal Loss: 0.4716, Val Acc: 0.7453\n\nEpoch 3/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1225, Train Acc: 0.9560\nVal Loss: 0.4168, Val Acc: 0.8003\n\nEpoch 4/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0644, Train Acc: 0.9787\nVal Loss: 0.5582, Val Acc: 0.7877\n\nEpoch 5/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0360, Train Acc: 0.9899\nVal Loss: 0.7007, Val Acc: 0.7827\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4862, Train Acc: 0.7529\nVal Loss: 0.5243, Val Acc: 0.7156\n\nEpoch 2/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2240, Train Acc: 0.9116\nVal Loss: 0.4545, Val Acc: 0.7640\n\nEpoch 3/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1319, Train Acc: 0.9525\nVal Loss: 0.4995, Val Acc: 0.7591\n\nEpoch 4/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0695, Train Acc: 0.9771\nVal Loss: 0.5050, Val Acc: 0.8042\n\nEpoch 5/5 | Learning Rate: 0.000024\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0414, Train Acc: 0.9868\nVal Loss: 0.4858, Val Acc: 0.8344\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 15:06:39,139] Trial 5 finished with value: 0.7931793179317932 and parameters: {'lr': 2.4067422144226153e-05}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6239, Train Acc: 0.6392\nVal Loss: 0.8071, Val Acc: 0.4180\n\nEpoch 2/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4811, Train Acc: 0.7696\nVal Loss: 0.6485, Val Acc: 0.6414\n\nEpoch 3/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3755, Train Acc: 0.8494\nVal Loss: 0.6241, Val Acc: 0.6601\n\nEpoch 4/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2944, Train Acc: 0.8876\nVal Loss: 0.5185, Val Acc: 0.7305\n\nEpoch 5/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2388, Train Acc: 0.9131\nVal Loss: 0.4858, Val Acc: 0.7448\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6174, Train Acc: 0.6616\nVal Loss: 0.8735, Val Acc: 0.2921\n\nEpoch 2/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4918, Train Acc: 0.7762\nVal Loss: 0.7809, Val Acc: 0.4758\n\nEpoch 3/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3952, Train Acc: 0.8439\nVal Loss: 0.6680, Val Acc: 0.6089\n\nEpoch 4/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3130, Train Acc: 0.8793\nVal Loss: 0.4986, Val Acc: 0.7332\n\nEpoch 5/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2501, Train Acc: 0.9092\nVal Loss: 0.4385, Val Acc: 0.7684\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7362, Train Acc: 0.5075\nVal Loss: 0.5920, Val Acc: 0.7052\n\nEpoch 2/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4913, Train Acc: 0.7826\nVal Loss: 0.6354, Val Acc: 0.6254\n\nEpoch 3/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3689, Train Acc: 0.8507\nVal Loss: 0.6178, Val Acc: 0.6414\n\nEpoch 4/5 | Learning Rate: 0.000006\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2874, Train Acc: 0.8887\nVal Loss: 0.5533, Val Acc: 0.6832\n\nEpoch 5/5 | Learning Rate: 0.000003\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2313, Train Acc: 0.9123\nVal Loss: 0.5549, Val Acc: 0.6887\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 15:20:52,938] Trial 6 finished with value: 0.733956729006234 and parameters: {'lr': 6.220903262461637e-06}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1\n\nFine-tuning model: DENSENET121 on Fold 1\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5262, Train Acc: 0.7269\nVal Loss: 0.6178, Val Acc: 0.6667\n\nEpoch 2/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2829, Train Acc: 0.8852\nVal Loss: 0.4557, Val Acc: 0.7475\n\nEpoch 3/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1744, Train Acc: 0.9314\nVal Loss: 0.4682, Val Acc: 0.7580\n\nEpoch 4/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1088, Train Acc: 0.9622\nVal Loss: 0.4583, Val Acc: 0.7882\n\nEpoch 5/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0616, Train Acc: 0.9807\nVal Loss: 0.5586, Val Acc: 0.7816\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n\nFine-tuning model: DENSENET121 on Fold 2\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5172, Train Acc: 0.7361\nVal Loss: 0.6162, Val Acc: 0.6304\n\nEpoch 2/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2797, Train Acc: 0.8841\nVal Loss: 0.5325, Val Acc: 0.7140\n\nEpoch 3/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1687, Train Acc: 0.9380\nVal Loss: 0.4333, Val Acc: 0.7866\n\nEpoch 4/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1013, Train Acc: 0.9642\nVal Loss: 0.5035, Val Acc: 0.7811\n\nEpoch 5/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0643, Train Acc: 0.9789\nVal Loss: 0.5156, Val Acc: 0.8113\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n\nFine-tuning model: DENSENET121 on Fold 3\nTotal trainable parameters: 5,658,370\n\nEpoch 1/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4987, Train Acc: 0.7568\nVal Loss: 0.6467, Val Acc: 0.6260\n\nEpoch 2/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2777, Train Acc: 0.8894\nVal Loss: 0.4858, Val Acc: 0.7327\n\nEpoch 3/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1677, Train Acc: 0.9349\nVal Loss: 0.4039, Val Acc: 0.7943\n\nEpoch 4/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1104, Train Acc: 0.9607\nVal Loss: 0.5003, Val Acc: 0.7800\n\nEpoch 5/5 | Learning Rate: 0.000018\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0653, Train Acc: 0.9791\nVal Loss: 0.4024, Val Acc: 0.8339\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 15:35:10,034] Trial 7 finished with value: 0.8089475614228089 and parameters: {'lr': 1.7624322855256126e-05}. Best is trial 1 with value: 0.8912724605793914.\n","output_type":"stream"},{"name":"stdout","text":"\nBest Trial:\n  Value (Mean Validation Accuracy): 0.8912724605793914\n  Params: {'lr': 0.0008263872006727257}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# After the Optuna optimization process, we will use the best trial to set the learning rate","metadata":{}},{"cell_type":"code","source":"# Best learning rate from Optuna\nbest_lr = 0.0008263872006727257","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:44:01.311621Z","iopub.execute_input":"2024-12-25T15:44:01.311963Z","iopub.status.idle":"2024-12-25T15:44:01.315292Z","shell.execute_reply.started":"2024-12-25T15:44:01.311903Z","shell.execute_reply":"2024-12-25T15:44:01.314486Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"final_results = {}\n\nfor model_name in [\"densenet121\"]:\n    print(f\"\\nTraining model: {model_name.upper()} with best learning rate {best_lr}\")\n    \n    # Initialize model\n    model = get_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    \n    # Create DataLoaders (assuming combinedData is split into train and test datasets)\n    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\n    # Train the model\n    for epoch in range(10):  # Example: 10 epochs\n        print(f\"\\nEpoch {epoch + 1}/10\")\n        \n        # Train and evaluate\n        train_loss, train_acc = TrainingModels(model, train_loader, criterion, optimizer)\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    \n    # Final evaluation on the test set\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n    \n    # Store results\n    final_results[model_name] = {\n        \"Test Loss\": test_loss,\n        \"Test Accuracy\": test_acc\n    }\n\n# Print final results\nprint(\"\\nFinal Results:\")\nfor model_name, metrics in final_results.items():\n    print(f\"{model_name.upper()}:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:44:40.166256Z","iopub.execute_input":"2024-12-25T15:44:40.166532Z","iopub.status.idle":"2024-12-25T15:52:28.023933Z","shell.execute_reply.started":"2024-12-25T15:44:40.166510Z","shell.execute_reply":"2024-12-25T15:52:28.022972Z"}},"outputs":[{"name":"stdout","text":"\nTraining model: DENSENET121 with best learning rate 0.0008263872006727257\nTotal trainable parameters: 5,658,370\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3183, Train Acc: 0.8597\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1754, Train Acc: 0.9292\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1074, Train Acc: 0.9596\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0741, Train Acc: 0.9727\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0541, Train Acc: 0.9818\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0442, Train Acc: 0.9829\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0288, Train Acc: 0.9894\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0364, Train Acc: 0.9888\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0203, Train Acc: 0.9932\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0238, Train Acc: 0.9921\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3673, Test Acc: 0.9224\n\nFinal Results:\nDENSENET121:\n  Test Loss: 0.3673\n  Test Accuracy: 0.9224\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Function to evaluate the model and get performance metrics\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\n# Initialize cross-validation metrics storage\nconfusion_matrices = []\naccuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\n\nbest_lr = 0.0008263872006727257\n\nfor model_name in [\"densenet121\"]:\n    print(f\"\\nTraining model: {model_name.upper()} with best learning rate {best_lr}\")\n    \n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(np.arange(samples))):\n        print(f\"Fold {fold_idx + 1}\")\n\n        train_size = int(0.75 * len(train_val_idx))\n        train_idx = train_val_idx[:train_size]\n        val_idx = train_val_idx[train_size:]\n\n        train_set = Subset(combinedData, train_idx)\n        val_set = Subset(combinedData, val_idx)\n        test_set = Subset(combinedData, test_idx)\n\n        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n        \n        \n        model = get_model(model_name).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=best_lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # Train the model\n        for epoch in range(10):\n            model.train()\n            running_loss = 0.0\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}/10, Loss: {running_loss / len(train_loader):.4f}\")\n\n    \n        preds, labels = evaluate_model(model, test_loader)\n\n        \n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average='weighted', zero_division=0)\n        recall = recall_score(labels, preds, average='weighted', zero_division=0)\n        f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n        cm = confusion_matrix(labels, preds)\n\n        # Store metrics for the fold\n        confusion_matrices.append(cm)\n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n\n        print(f\"Fold {fold_idx + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n        print(f\"Confusion Matrix:\\n{cm}\\n\")\n\n    # Average metrics across folds\n    avg_accuracy = np.mean(accuracies)\n    avg_precision = np.mean(precisions)\n    avg_recall = np.mean(recalls)\n    avg_f1_score = np.mean(f1_scores)\n    avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n\n    # Print final average metrics for the model\n    print(f\"\\nFinal Results for {model_name.upper()}:\")\n    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n    print(f\"Average Precision: {avg_precision:.4f}\")\n    print(f\"Average Recall: {avg_recall:.4f}\")\n    print(f\"Average F1-Score: {avg_f1_score:.4f}\")\n    print(f\"Average Confusion Matrix:\\n{avg_confusion_matrix}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:52:31.787161Z","iopub.execute_input":"2024-12-25T15:52:31.787465Z","iopub.status.idle":"2024-12-25T16:15:25.677825Z","shell.execute_reply.started":"2024-12-25T15:52:31.787438Z","shell.execute_reply":"2024-12-25T16:15:25.676879Z"}},"outputs":[{"name":"stdout","text":"\nTraining model: DENSENET121 with best learning rate 0.0008263872006727257\nFold 1\nTotal trainable parameters: 5,658,370\nEpoch 1/10, Loss: 0.3023\nEpoch 2/10, Loss: 0.1586\nEpoch 3/10, Loss: 0.0971\nEpoch 4/10, Loss: 0.0824\nEpoch 5/10, Loss: 0.0542\nEpoch 6/10, Loss: 0.0532\nEpoch 7/10, Loss: 0.0358\nEpoch 8/10, Loss: 0.0386\nEpoch 9/10, Loss: 0.0292\nEpoch 10/10, Loss: 0.0277\nFold 1 - Accuracy: 0.9208, Precision: 0.9254, Recall: 0.9208, F1-Score: 0.9204\nConfusion Matrix:\n[[1817   47]\n [ 241 1530]]\n\nFold 2\nTotal trainable parameters: 5,658,370\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2996\nEpoch 2/10, Loss: 0.1700\nEpoch 3/10, Loss: 0.0987\nEpoch 4/10, Loss: 0.0681\nEpoch 5/10, Loss: 0.0468\nEpoch 6/10, Loss: 0.0398\nEpoch 7/10, Loss: 0.0525\nEpoch 8/10, Loss: 0.0227\nEpoch 9/10, Loss: 0.0213\nEpoch 10/10, Loss: 0.0313\nFold 2 - Accuracy: 0.9232, Precision: 0.9280, Recall: 0.9232, F1-Score: 0.9230\nConfusion Matrix:\n[[1791   43]\n [ 236 1565]]\n\nFold 3\nTotal trainable parameters: 5,658,370\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.2993\nEpoch 2/10, Loss: 0.1568\nEpoch 3/10, Loss: 0.1068\nEpoch 4/10, Loss: 0.0836\nEpoch 5/10, Loss: 0.0494\nEpoch 6/10, Loss: 0.0582\nEpoch 7/10, Loss: 0.0304\nEpoch 8/10, Loss: 0.0257\nEpoch 9/10, Loss: 0.0409\nEpoch 10/10, Loss: 0.0224\nFold 3 - Accuracy: 0.9387, Precision: 0.9391, Recall: 0.9387, F1-Score: 0.9386\nConfusion Matrix:\n[[1710   84]\n [ 139 1702]]\n\n\nFinal Results for DENSENET121:\nAverage Accuracy: 0.9276\nAverage Precision: 0.9308\nAverage Recall: 0.9276\nAverage F1-Score: 0.9274\nAverage Confusion Matrix:\n[[1772.66666667   58.        ]\n [ 205.33333333 1599.        ]]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}